{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplacian Eigenmaps\n",
    "\n",
    "\n",
    "Demonstration of using the method of [1] for representation learning on graphs. The node representations are used to perform node attribute inference on a paper citation network, namely Cora. \n",
    "\n",
    "**References**\n",
    "\n",
    "[1] Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering, M. Belkin and P.Niyogi, NIPS 2002\n",
    "\n",
    "Copyright 2010-2019 Commonwealth Scientific and Industrial Research Organisation (CSIRO).\n",
    "\n",
    "All Rights Reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "### Dataset\n",
    "\n",
    "\n",
    "The dataset is the citation network Cora.\n",
    "\n",
    "It can be downloaded by clicking [here](https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz)\n",
    "\n",
    "The following is the description of the dataset from the publisher,\n",
    "\n",
    "> The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words. The README file in the dataset provides more details. \n",
    "\n",
    "For this demo, we ignore the word vectors associated with each paper. We are only interested in the network structure and the **subject** attribute of each paper.\n",
    "\n",
    "Download and unzip the cora.tgz file to a location on your computer. \n",
    "\n",
    "We assume that the dataset is stored in the directory\n",
    "\n",
    "`../data/cora/`\n",
    "\n",
    "where the files `cora.cites` and `cora.content` can be located.\n",
    "\n",
    "We are going to load the data into a networkx object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the data input directory\n",
    "\n",
    "**Note:** Make sure this is set correctly on your machine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = os.path.expanduser(\"../data/cora/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cora_location = os.path.expanduser(os.path.join(data_dir, \"cora.cites\"))\n",
    "g_nx = nx.read_edgelist(path=cora_location)\n",
    "\n",
    "# load the node attribute data\n",
    "cora_data_location = os.path.expanduser(os.path.join(data_dir, \"cora.content\"))\n",
    "node_attr = pd.read_csv(cora_data_location, sep='\\t', header=None)\n",
    "values = { str(row.tolist()[0]): row.tolist()[-1] for _, row in node_attr.iterrows()}\n",
    "nx.set_node_attributes(g_nx, values, 'subject')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the features and subject for the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_names = [\"w_{}\".format(ii) for ii in range(1433)]\n",
    "column_names =  feature_names + [\"subject\"]\n",
    "node_data = pd.read_table(os.path.join(data_dir, \"cora.content\"), header=None, names=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use only the largest graph connected component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the largest connected component. For clarity we ignore isolated\n",
    "# nodes and subgraphs; having these in the data does not prevent the\n",
    "# algorithm from running and producing valid results.\n",
    "g_nx_ccs = (g_nx.subgraph(c).copy() for c in nx.connected_components(g_nx))\n",
    "g_nx = max(g_nx_ccs, key=len)\n",
    "print(\"Largest subgraph statistics: {} nodes, {} edges\".format(\n",
    "    g_nx.number_of_nodes(), g_nx.number_of_edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the labels for the nodes in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_ids = list(g_nx.nodes())\n",
    "node_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_targets = [ g_nx.nodes[node_id]['subject'] for node_id in node_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_targets[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(node_targets) # should be 1 of 7 subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(node_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(g_nx.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = node_targets\n",
    "y[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the feature vectors for the nodes in the largest connected component.\n",
    "\n",
    "We won't use these at first but we might use them later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "node_data.drop([\"subject\"], inplace=True, axis=1)  # drop the subject column\n",
    "node_data.index = node_data.index.map(str)  # make sure the index is string because the graph uses string for node ids\n",
    "\n",
    "node_data = node_data[node_data.index.isin(list(g_nx.nodes()))]  # get the rows for the nodes in the graph\n",
    "\n",
    "node_data = node_data.reindex(list(g_nx.nodes()))# reindex so that the order of the features is the same as the order of\n",
    "                                                 # the nodes in the graph and the order of the nodes in the adjacency\n",
    "                                                 # matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We are going to use these to draw the data such that nodes with the same subject have the\n",
    "# same color.\n",
    "colors = {'Case_Based': 'black',\n",
    "          'Genetic_Algorithms': 'red',\n",
    "          'Neural_Networks': 'blue',\n",
    "          'Probabilistic_Methods': 'green',\n",
    "          'Reinforcement_Learning': 'aqua',\n",
    "          'Rule_Learning': 'purple',\n",
    "          'Theory': 'yellow'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the graph Laplacian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 graph Laplacians commonly used. These are called unormalized, random walk and normalised graph Laplacian and they are defined as follows:\n",
    "\n",
    "Unormalised: $L = D-A$\n",
    "\n",
    "Random Walk: $L_{rw} = D^{-1}L = I - D^{-1}A$\n",
    "\n",
    "Normalised:  $L_{sym} = D^{-1/2}LD^{-1/2} = I - D^{-1/2}AD^{-1/2}$\n",
    "\n",
    "We are going to consider the unormalised graph Laplacian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First retrieve the adjacency matrix from the networkx object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = nx.to_numpy_array(g_nx)  # The graph adjacency matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the degree matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.diag(A.sum(axis=1))  # sum rows\n",
    "\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L = D-A  # The Graph Laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the eigenvectors and corresponding eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w, v = np.linalg.eig(L)  # w is vector of eigenvalues\n",
    "                         # v columns are eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Just use the real parts of w and v\n",
    "w = np.real(w)\n",
    "v = np.real(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "order = np.argsort(w)  # from smallest to largest eigenvalue\n",
    "w = w[order]\n",
    "v_0 = v[:, order[0]]\n",
    "v = v[:, order[1:(embedding_size+1)]]  # ignore the first one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2)  # use PCA for speed\n",
    "v_pr = tsne.fit_transform(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_pr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the points\n",
    "alpha=0.7\n",
    "label_map = { l: i for i, l in enumerate(np.unique(node_targets))}\n",
    "node_colours = [ label_map[target] for target in node_targets]\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.scatter(v_pr[:,0], \n",
    "            v_pr[:,1], \n",
    "            c=node_colours, cmap=\"jet\", alpha=alpha)\n",
    "# fig.savefig(\"LE32_embeddings.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = v\n",
    "Y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some other suitable classifiers\n",
    "\n",
    "Other than Random Forrest classification, one can use any of the following,\n",
    "\n",
    "[Logistic Regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)\n",
    "[Support Vector Classification](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)\n",
    "[Nearest Neighbors](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=10, min_samples_leaf=4)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=140, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"score on X_train {}\".format(clf.score(X_train, y_train)))\n",
    "print(\"score on X_test {}\".format(clf.score(X_test, y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practical-ml",
   "language": "python",
   "name": "practical-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
